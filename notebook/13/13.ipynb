{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e4dc2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "import ast\n",
    "import wget\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import timm\n",
    "from timm.models.layers import to_2tuple,trunc_normal_\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as torchdata\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from timm.models.layers import SelectAdaptivePool2d\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torchlibrosa.stft import LogmelFilterBank, Spectrogram\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "import torchaudio\n",
    "\n",
    "# from transformers import AdamW\n",
    "# from transformers import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8144076",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../')\n",
    "import src.utils as utils\n",
    "from config_ import CFG\n",
    "\n",
    "os.environ['TORCH_HOME'] = '../../src/pretrained_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cdfd435-d447-4457-9147-abfcc7025059",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e6a0f6",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8cfbaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"/home/knikaido/work/BirdCLEF2021/data/\")\n",
    "MAIN_DATA_DIR = DATA_DIR / 'birdclef-2021'\n",
    "OUTPUT_DIR = Path('./output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35e1d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Data #\n",
    "######################\n",
    "train_datadir = MAIN_DATA_DIR / 'train_short_audio'\n",
    "train_csv = MAIN_DATA_DIR / 'train_metadata.csv'\n",
    "train_soundscape = MAIN_DATA_DIR / 'train_soundscape_labels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d500d890",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    CFG.epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e8a645",
   "metadata": {},
   "source": [
    "## My func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28867de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveformDataset(torchdata.Dataset):\n",
    "    def __init__(self,\n",
    "                 df: pd.DataFrame,\n",
    "                 datadir: Path,\n",
    "                 img_size=224,\n",
    "                 period=20,\n",
    "                 validation=False):\n",
    "        self.df = df\n",
    "        self.datadir = datadir\n",
    "        self.img_size = img_size\n",
    "        self.period = period\n",
    "        self.validation = validation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        sample = self.df.loc[idx, :]\n",
    "        wav_name = sample[\"filename\"]\n",
    "        ebird_code = sample[\"primary_label\"]\n",
    "        primary_label = sample[\"primary_labels\"]\n",
    "        secondary_labels = sample[\"secondary_labels\"]\n",
    "\n",
    "        y, sr = sf.read(self.datadir / ebird_code / wav_name)\n",
    "\n",
    "        len_y = len(y)\n",
    "        effective_length = sr * self.period\n",
    "        if len_y < effective_length:\n",
    "            new_y = np.zeros(effective_length, dtype=y.dtype)\n",
    "            if not self.validation:\n",
    "                start = np.random.randint(effective_length - len_y)\n",
    "            else:\n",
    "                start = 0\n",
    "            new_y[start:start + len_y] = y\n",
    "            y = new_y.astype(np.float32)\n",
    "        elif len_y > effective_length:\n",
    "            if not self.validation:\n",
    "                start = np.random.randint(len_y - effective_length)\n",
    "            else:\n",
    "                start = 0\n",
    "            y = y[start:start + effective_length].astype(np.float32)\n",
    "        else:\n",
    "            y = y.astype(np.float32)\n",
    "\n",
    "        y = np.nan_to_num(y)\n",
    "\n",
    "        labels = primary_label\n",
    "        labels = labels.astype(np.float32)\n",
    "\n",
    "        return {\n",
    "            \"image\": y,\n",
    "            \"targets\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602abc9c",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "450729be-6950-48ab-b134-51d60d54547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class ASTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The AST model.\n",
    "    :param label_dim: the label dimension, i.e., the number of total classes, it is 527 for AudioSet, 50 for ESC-50, and 35 for speechcommands v2-35\n",
    "    :param fstride: the stride of patch spliting on the frequency dimension, for 16*16 patchs, fstride=16 means no overlap, fstride=10 means overlap of 6\n",
    "    :param tstride: the stride of patch spliting on the time dimension, for 16*16 patchs, tstride=16 means no overlap, tstride=10 means overlap of 6\n",
    "    :param input_fdim: the number of frequency bins of the input spectrogram\n",
    "    :param input_tdim: the number of time frames of the input spectrogram\n",
    "    :param imagenet_pretrain: if use ImageNet pretrained model\n",
    "    :param audioset_pretrain: if use full AudioSet and ImageNet pretrained model\n",
    "    :param model_size: the model size of AST, should be in [tiny224, small224, base224, base384], base224 and base 384 are same model, but are trained differently during ImageNet pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self, label_dim=527, fstride=10, tstride=10, input_fdim=128, input_tdim=1024, imagenet_pretrain=True, audioset_pretrain=False, model_size='base384', verbose=True):\n",
    "\n",
    "        super(ASTModel, self).__init__()\n",
    "#         assert timm.__version__ == '0.4.5', 'Please use timm == 0.4.5, the code might not be compatible with newer versions.'\n",
    "        self.melspectrogramer = torchaudio.transforms.MelSpectrogram(sample_rate=CFG.sample_rate, n_fft=CFG.n_fft, win_length=CFG.n_fft, \n",
    "                                                                     hop_length=CFG.hop_length, f_min=CFG.fmin, \n",
    "                                                                     f_max=CFG.fmax, n_mels=CFG.n_mels, power=1.0)\n",
    "        self.amplituder = torchaudio.transforms.AmplitudeToDB()\n",
    "        if verbose == True:\n",
    "            print('---------------AST Model Summary---------------')\n",
    "            print('ImageNet pretraining: {:s}, AudioSet pretraining: {:s}'.format(str(imagenet_pretrain),str(audioset_pretrain)))\n",
    "        # override timm input shape restriction\n",
    "        timm.models.vision_transformer.PatchEmbed = PatchEmbed\n",
    "\n",
    "        # if AudioSet pretraining is not used (but ImageNet pretraining may still apply)\n",
    "        if audioset_pretrain == False:\n",
    "            if model_size == 'tiny224':\n",
    "                self.v = timm.create_model('vit_deit_tiny_distilled_patch16_224', pretrained=imagenet_pretrain)\n",
    "            elif model_size == 'small224':\n",
    "                self.v = timm.create_model('vit_deit_small_distilled_patch16_224', pretrained=imagenet_pretrain)\n",
    "            elif model_size == 'base224':\n",
    "                self.v = timm.create_model('vit_deit_base_distilled_patch16_224', pretrained=imagenet_pretrain)\n",
    "            elif model_size == 'base384':\n",
    "                self.v = timm.create_model('vit_deit_base_distilled_patch16_384', pretrained=imagenet_pretrain)\n",
    "            else:\n",
    "                raise Exception('Model size must be one of tiny224, small224, base224, base384.')\n",
    "            self.original_num_patches = self.v.patch_embed.num_patches\n",
    "            self.oringal_hw = int(self.original_num_patches ** 0.5)\n",
    "            self.original_embedding_dim = self.v.pos_embed.shape[2]\n",
    "            self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n",
    "\n",
    "            # automatcially get the intermediate shape\n",
    "            f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n",
    "            num_patches = f_dim * t_dim\n",
    "            self.v.patch_embed.num_patches = num_patches\n",
    "            if verbose == True:\n",
    "                print('frequncey stride={:d}, time stride={:d}'.format(fstride, tstride))\n",
    "                print('number of patches={:d}'.format(num_patches))\n",
    "\n",
    "            # the linear projection layer\n",
    "            new_proj = torch.nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n",
    "            if imagenet_pretrain == True:\n",
    "                new_proj.weight = torch.nn.Parameter(torch.sum(self.v.patch_embed.proj.weight, dim=1).unsqueeze(1))\n",
    "                new_proj.bias = self.v.patch_embed.proj.bias\n",
    "            self.v.patch_embed.proj = new_proj\n",
    "\n",
    "            # the positional embedding\n",
    "            if imagenet_pretrain == True:\n",
    "                # get the positional embedding from deit model, skip the first two tokens (cls token and distillation token), reshape it to original 2D shape (24*24).\n",
    "                new_pos_embed = self.v.pos_embed[:, 2:, :].detach().reshape(1, self.original_num_patches, self.original_embedding_dim).transpose(1, 2).reshape(1, self.original_embedding_dim, self.oringal_hw, self.oringal_hw)\n",
    "                # cut (from middle) or interpolate the second dimension of the positional embedding\n",
    "                if t_dim <= self.oringal_hw:\n",
    "                    new_pos_embed = new_pos_embed[:, :, :, int(self.oringal_hw / 2) - int(t_dim / 2): int(self.oringal_hw / 2) - int(t_dim / 2) + t_dim]\n",
    "                else:\n",
    "                    new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(self.oringal_hw, t_dim), mode='bilinear')\n",
    "                # cut (from middle) or interpolate the first dimension of the positional embedding\n",
    "                if f_dim <= self.oringal_hw:\n",
    "                    new_pos_embed = new_pos_embed[:, :, int(self.oringal_hw / 2) - int(f_dim / 2): int(self.oringal_hw / 2) - int(f_dim / 2) + f_dim, :]\n",
    "                else:\n",
    "                    new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(f_dim, t_dim), mode='bilinear')\n",
    "                # flatten the positional embedding\n",
    "                new_pos_embed = new_pos_embed.reshape(1, self.original_embedding_dim, num_patches).transpose(1,2)\n",
    "                # concatenate the above positional embedding with the cls token and distillation token of the deit model.\n",
    "                self.v.pos_embed = nn.Parameter(torch.cat([self.v.pos_embed[:, :2, :].detach(), new_pos_embed], dim=1))\n",
    "            else:\n",
    "                # if not use imagenet pretrained model, just randomly initialize a learnable positional embedding\n",
    "                # TODO can use sinusoidal positional embedding instead\n",
    "                new_pos_embed = nn.Parameter(torch.zeros(1, self.v.patch_embed.num_patches + 2, self.original_embedding_dim))\n",
    "                self.v.pos_embed = new_pos_embed\n",
    "                trunc_normal_(self.v.pos_embed, std=.02)\n",
    "\n",
    "        # now load a model that is pretrained on both ImageNet and AudioSet\n",
    "        elif audioset_pretrain == True:\n",
    "            if audioset_pretrain == True and imagenet_pretrain == False:\n",
    "                raise ValueError('currently model pretrained on only audioset is not supported, please set imagenet_pretrain = True to use audioset pretrained model.')\n",
    "            if model_size != 'base384':\n",
    "                raise ValueError('currently only has base384 AudioSet pretrained model.')\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            if os.path.exists('../../src/pretrained_models/audioset_10_10_0.4593.pth') == False:\n",
    "                # this model performs 0.4593 mAP on the audioset eval set\n",
    "                audioset_mdl_url = 'https://www.dropbox.com/s/cv4knew8mvbrnvq/audioset_0.4593.pth?dl=1'\n",
    "                wget.download(audioset_mdl_url, out='../../src/pretrained_models/audioset_10_10_0.4593.pth')\n",
    "            sd = torch.load('../../src/pretrained_models/audioset_10_10_0.4593.pth', map_location=device)\n",
    "            audio_model = ASTModel(label_dim=527, fstride=10, tstride=10, input_fdim=128, input_tdim=1024, imagenet_pretrain=False, audioset_pretrain=False, model_size='base384', verbose=False)\n",
    "            audio_model = torch.nn.DataParallel(audio_model)\n",
    "            audio_model.load_state_dict(sd, strict=False)\n",
    "            self.v = audio_model.module.v\n",
    "            self.original_embedding_dim = self.v.pos_embed.shape[2]\n",
    "            self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n",
    "\n",
    "            f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n",
    "            num_patches = f_dim * t_dim\n",
    "            self.v.patch_embed.num_patches = num_patches\n",
    "            if verbose == True:\n",
    "                print('frequncey stride={:d}, time stride={:d}'.format(fstride, tstride))\n",
    "                print('number of patches={:d}'.format(num_patches))\n",
    "\n",
    "            new_pos_embed = self.v.pos_embed[:, 2:, :].detach().reshape(1, 1212, 768).transpose(1, 2).reshape(1, 768, 12, 101)\n",
    "            # if the input sequence length is larger than the original audioset (10s), then cut the positional embedding\n",
    "            if t_dim < 101:\n",
    "                new_pos_embed = new_pos_embed[:, :, :, 50 - int(t_dim/2): 50 - int(t_dim/2) + t_dim]\n",
    "            # otherwise interpolate\n",
    "            else:\n",
    "                new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(12, t_dim), mode='bilinear')\n",
    "            new_pos_embed = new_pos_embed.reshape(1, 768, num_patches).transpose(1, 2)\n",
    "            self.v.pos_embed = nn.Parameter(torch.cat([self.v.pos_embed[:, :2, :].detach(), new_pos_embed], dim=1))\n",
    "\n",
    "    def get_shape(self, fstride, tstride, input_fdim=128, input_tdim=1024):\n",
    "        test_input = torch.randn(1, 1, input_fdim, input_tdim)\n",
    "        test_proj = nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n",
    "        test_out = test_proj(test_input)\n",
    "        f_dim = test_out.shape[2]\n",
    "        t_dim = test_out.shape[3]\n",
    "        return f_dim, t_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: the input spectrogram, expected shape: (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
    "        :return: prediction\n",
    "        \"\"\"\n",
    "        x = self.melspectrogramer(x)\n",
    "        x = self.amplituder(x)\n",
    "        # expect input x = (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
    "        x = x.unsqueeze(1)\n",
    "        x_mean = x.mean(keepdim=True, dim=(1, 2, 3))\n",
    "        x_std = x.std(keepdim=True, dim=(1, 2, 3))\n",
    "        x = (x - x_mean) / (x_std * 2 + 1e-6)\n",
    "        \n",
    "        B = x.shape[0]\n",
    "        x = self.v.patch_embed(x)\n",
    "        cls_tokens = self.v.cls_token.expand(B, -1, -1)\n",
    "        dist_token = self.v.dist_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n",
    "        x = x + self.v.pos_embed\n",
    "        x = self.v.pos_drop(x)\n",
    "        for blk in self.v.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.v.norm(x)\n",
    "        x = (x[:, 0] + x[:, 1]) / 2\n",
    "\n",
    "        x = self.mlp_head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569c98ce",
   "metadata": {},
   "source": [
    "## Training Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f4a6430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom optimizer\n",
    "__OPTIMIZERS__ = {}\n",
    "\n",
    "\n",
    "def get_optimizer(model: nn.Module):\n",
    "    optimizer_name = CFG.optimizer_name\n",
    "    if optimizer_name == \"SAM\":\n",
    "        base_optimizer_name = CFG.base_optimizer\n",
    "        if __OPTIMIZERS__.get(base_optimizer_name) is not None:\n",
    "            base_optimizer = __OPTIMIZERS__[base_optimizer_name]\n",
    "        else:\n",
    "            base_optimizer = optim.__getattribute__(base_optimizer_name)\n",
    "        return SAM(model.parameters(), base_optimizer, **CFG.optimizer_params)\n",
    "\n",
    "    if __OPTIMIZERS__.get(optimizer_name) is not None:\n",
    "        return __OPTIMIZERS__[optimizer_name](model.parameters(),\n",
    "                                              **CFG.optimizer_params)\n",
    "    else:\n",
    "        return optim.__getattribute__(optimizer_name)(model.parameters(),\n",
    "                                                      **CFG.optimizer_params)\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer):\n",
    "    scheduler_name = CFG.scheduler_name\n",
    "\n",
    "    if scheduler_name is None:\n",
    "        return\n",
    "    else:\n",
    "        return optim.lr_scheduler.__getattribute__(scheduler_name)(\n",
    "            optimizer, **CFG.scheduler_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd034b2",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c44932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "utils.set_seed(CFG.seed)\n",
    "\n",
    "# validation\n",
    "splitter = getattr(model_selection, CFG.split)(**CFG.split_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "739b3612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary_label</th>\n",
       "      <th>secondary_labels</th>\n",
       "      <th>type</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>scientific_name</th>\n",
       "      <th>common_name</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>filename</th>\n",
       "      <th>license</th>\n",
       "      <th>rating</th>\n",
       "      <th>time</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acafly</td>\n",
       "      <td>[amegfi]</td>\n",
       "      <td>['begging call', 'call', 'juvenile']</td>\n",
       "      <td>35.3860</td>\n",
       "      <td>-84.1250</td>\n",
       "      <td>Empidonax virescens</td>\n",
       "      <td>Acadian Flycatcher</td>\n",
       "      <td>Mike Nelson</td>\n",
       "      <td>2012-08-12</td>\n",
       "      <td>XC109605.ogg</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>09:30</td>\n",
       "      <td>https://www.xeno-canto.org/109605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acafly</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call']</td>\n",
       "      <td>9.1334</td>\n",
       "      <td>-79.6501</td>\n",
       "      <td>Empidonax virescens</td>\n",
       "      <td>Acadian Flycatcher</td>\n",
       "      <td>Allen T. Chartier</td>\n",
       "      <td>2000-12-26</td>\n",
       "      <td>XC11209.ogg</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>?</td>\n",
       "      <td>https://www.xeno-canto.org/11209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acafly</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call']</td>\n",
       "      <td>5.7813</td>\n",
       "      <td>-75.7452</td>\n",
       "      <td>Empidonax virescens</td>\n",
       "      <td>Acadian Flycatcher</td>\n",
       "      <td>Sergio Chaparro-Herrera</td>\n",
       "      <td>2012-01-10</td>\n",
       "      <td>XC127032.ogg</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15:20</td>\n",
       "      <td>https://www.xeno-canto.org/127032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acafly</td>\n",
       "      <td>[whwbec1]</td>\n",
       "      <td>['call']</td>\n",
       "      <td>4.6717</td>\n",
       "      <td>-75.6283</td>\n",
       "      <td>Empidonax virescens</td>\n",
       "      <td>Acadian Flycatcher</td>\n",
       "      <td>Oscar Humberto Marin-Gomez</td>\n",
       "      <td>2009-06-19</td>\n",
       "      <td>XC129974.ogg</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>07:50</td>\n",
       "      <td>https://www.xeno-canto.org/129974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acafly</td>\n",
       "      <td>[whwbec1]</td>\n",
       "      <td>['call']</td>\n",
       "      <td>4.6717</td>\n",
       "      <td>-75.6283</td>\n",
       "      <td>Empidonax virescens</td>\n",
       "      <td>Acadian Flycatcher</td>\n",
       "      <td>Oscar Humberto Marin-Gomez</td>\n",
       "      <td>2009-06-19</td>\n",
       "      <td>XC129981.ogg</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>07:50</td>\n",
       "      <td>https://www.xeno-canto.org/129981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62869</th>\n",
       "      <td>yetvir</td>\n",
       "      <td>[]</td>\n",
       "      <td>['adult', 'male', 'song']</td>\n",
       "      <td>30.2150</td>\n",
       "      <td>-97.6505</td>\n",
       "      <td>Vireo flavifrons</td>\n",
       "      <td>Yellow-throated Vireo</td>\n",
       "      <td>Caleb Helsel</td>\n",
       "      <td>2020-07-10</td>\n",
       "      <td>XC591680.ogg</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>08:30</td>\n",
       "      <td>https://www.xeno-canto.org/591680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62870</th>\n",
       "      <td>yetvir</td>\n",
       "      <td>[]</td>\n",
       "      <td>['life stage uncertain', 'sex uncertain', 'song']</td>\n",
       "      <td>42.3005</td>\n",
       "      <td>-72.5877</td>\n",
       "      <td>Vireo flavifrons</td>\n",
       "      <td>Yellow-throated Vireo</td>\n",
       "      <td>Christopher McPherson</td>\n",
       "      <td>2019-05-31</td>\n",
       "      <td>XC600085.ogg</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>09:30</td>\n",
       "      <td>https://www.xeno-canto.org/600085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62871</th>\n",
       "      <td>yetvir</td>\n",
       "      <td>[amered, eawpew, norcar, reevir1]</td>\n",
       "      <td>['adult', 'male', 'song']</td>\n",
       "      <td>42.3005</td>\n",
       "      <td>-72.5877</td>\n",
       "      <td>Vireo flavifrons</td>\n",
       "      <td>Yellow-throated Vireo</td>\n",
       "      <td>Christopher McPherson</td>\n",
       "      <td>2020-06-02</td>\n",
       "      <td>XC602701.ogg</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>08:30</td>\n",
       "      <td>https://www.xeno-canto.org/602701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62872</th>\n",
       "      <td>yetvir</td>\n",
       "      <td>[]</td>\n",
       "      <td>['uncertain']</td>\n",
       "      <td>32.2357</td>\n",
       "      <td>-99.8811</td>\n",
       "      <td>Vireo flavifrons</td>\n",
       "      <td>Yellow-throated Vireo</td>\n",
       "      <td>Brad Banner</td>\n",
       "      <td>2019-04-27</td>\n",
       "      <td>XC614733.ogg</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17:30</td>\n",
       "      <td>https://www.xeno-canto.org/614733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62873</th>\n",
       "      <td>yetvir</td>\n",
       "      <td>[gamqua, whwdov]</td>\n",
       "      <td>['adult', 'male', 'song']</td>\n",
       "      <td>31.9060</td>\n",
       "      <td>-109.1543</td>\n",
       "      <td>Vireo flavifrons</td>\n",
       "      <td>Yellow-throated Vireo</td>\n",
       "      <td>Richard E. Webster</td>\n",
       "      <td>2020-05-26</td>\n",
       "      <td>XC615888.ogg</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>06:23</td>\n",
       "      <td>https://www.xeno-canto.org/615888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62874 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      primary_label                   secondary_labels  \\\n",
       "0            acafly                           [amegfi]   \n",
       "1            acafly                                 []   \n",
       "2            acafly                                 []   \n",
       "3            acafly                          [whwbec1]   \n",
       "4            acafly                          [whwbec1]   \n",
       "...             ...                                ...   \n",
       "62869        yetvir                                 []   \n",
       "62870        yetvir                                 []   \n",
       "62871        yetvir  [amered, eawpew, norcar, reevir1]   \n",
       "62872        yetvir                                 []   \n",
       "62873        yetvir                   [gamqua, whwdov]   \n",
       "\n",
       "                                                    type  latitude  longitude  \\\n",
       "0                   ['begging call', 'call', 'juvenile']   35.3860   -84.1250   \n",
       "1                                               ['call']    9.1334   -79.6501   \n",
       "2                                               ['call']    5.7813   -75.7452   \n",
       "3                                               ['call']    4.6717   -75.6283   \n",
       "4                                               ['call']    4.6717   -75.6283   \n",
       "...                                                  ...       ...        ...   \n",
       "62869                          ['adult', 'male', 'song']   30.2150   -97.6505   \n",
       "62870  ['life stage uncertain', 'sex uncertain', 'song']   42.3005   -72.5877   \n",
       "62871                          ['adult', 'male', 'song']   42.3005   -72.5877   \n",
       "62872                                      ['uncertain']   32.2357   -99.8811   \n",
       "62873                          ['adult', 'male', 'song']   31.9060  -109.1543   \n",
       "\n",
       "           scientific_name            common_name                      author  \\\n",
       "0      Empidonax virescens     Acadian Flycatcher                 Mike Nelson   \n",
       "1      Empidonax virescens     Acadian Flycatcher           Allen T. Chartier   \n",
       "2      Empidonax virescens     Acadian Flycatcher     Sergio Chaparro-Herrera   \n",
       "3      Empidonax virescens     Acadian Flycatcher  Oscar Humberto Marin-Gomez   \n",
       "4      Empidonax virescens     Acadian Flycatcher  Oscar Humberto Marin-Gomez   \n",
       "...                    ...                    ...                         ...   \n",
       "62869     Vireo flavifrons  Yellow-throated Vireo                Caleb Helsel   \n",
       "62870     Vireo flavifrons  Yellow-throated Vireo       Christopher McPherson   \n",
       "62871     Vireo flavifrons  Yellow-throated Vireo       Christopher McPherson   \n",
       "62872     Vireo flavifrons  Yellow-throated Vireo                 Brad Banner   \n",
       "62873     Vireo flavifrons  Yellow-throated Vireo          Richard E. Webster   \n",
       "\n",
       "             date      filename  \\\n",
       "0      2012-08-12  XC109605.ogg   \n",
       "1      2000-12-26   XC11209.ogg   \n",
       "2      2012-01-10  XC127032.ogg   \n",
       "3      2009-06-19  XC129974.ogg   \n",
       "4      2009-06-19  XC129981.ogg   \n",
       "...           ...           ...   \n",
       "62869  2020-07-10  XC591680.ogg   \n",
       "62870  2019-05-31  XC600085.ogg   \n",
       "62871  2020-06-02  XC602701.ogg   \n",
       "62872  2019-04-27  XC614733.ogg   \n",
       "62873  2020-05-26  XC615888.ogg   \n",
       "\n",
       "                                                 license  rating   time  \\\n",
       "0      Creative Commons Attribution-NonCommercial-Sha...     2.5  09:30   \n",
       "1      Creative Commons Attribution-NonCommercial-Sha...     3.0      ?   \n",
       "2      Creative Commons Attribution-NonCommercial-Sha...     3.0  15:20   \n",
       "3      Creative Commons Attribution-NonCommercial-Sha...     3.5  07:50   \n",
       "4      Creative Commons Attribution-NonCommercial-Sha...     3.5  07:50   \n",
       "...                                                  ...     ...    ...   \n",
       "62869  Creative Commons Attribution-NonCommercial-Sha...     1.0  08:30   \n",
       "62870  Creative Commons Attribution-NonCommercial-Sha...     5.0  09:30   \n",
       "62871  Creative Commons Attribution-NonCommercial-Sha...     4.5  08:30   \n",
       "62872  Creative Commons Attribution-NonCommercial-Sha...     4.0  17:30   \n",
       "62873  Creative Commons Attribution-NonCommercial-Sha...     4.5  06:23   \n",
       "\n",
       "                                     url  \n",
       "0      https://www.xeno-canto.org/109605  \n",
       "1       https://www.xeno-canto.org/11209  \n",
       "2      https://www.xeno-canto.org/127032  \n",
       "3      https://www.xeno-canto.org/129974  \n",
       "4      https://www.xeno-canto.org/129981  \n",
       "...                                  ...  \n",
       "62869  https://www.xeno-canto.org/591680  \n",
       "62870  https://www.xeno-canto.org/600085  \n",
       "62871  https://www.xeno-canto.org/602701  \n",
       "62872  https://www.xeno-canto.org/614733  \n",
       "62873  https://www.xeno-canto.org/615888  \n",
       "\n",
       "[62874 rows x 14 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(MAIN_DATA_DIR / 'train_metadata.csv')\n",
    "train['secondary_labels'] = [ast.literal_eval(d) for d in train['secondary_labels']]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3beb172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_total = []\n",
    "for ebird_code in train['primary_label']:\n",
    "    labels = np.zeros(len(CFG.target_columns), dtype=float)\n",
    "    labels[CFG.target_columns.index(ebird_code)] = 1.0\n",
    "    labels_total.append(labels)\n",
    "\n",
    "train['primary_labels'] = labels_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12d82f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learner class(pytorch-lighting)\n",
    "class Learner(pl.LightningModule):\n",
    "    def __init__(self, model, num_train_steps, num_warmup_steps):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.num_train_steps = num_train_steps\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        b_data = batch\n",
    "        output = self.model(b_data['image'])\n",
    "        loss = self.criterion(output, b_data[\"targets\"])\n",
    "        \n",
    "        self.log(f'Loss/train', loss, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "#         sch = self.lr_schedulers()\n",
    "#         sch.step()\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        b_data = batch\n",
    "        output = self.model(b_data['image'])\n",
    "        loss = self.criterion(output, b_data[\"targets\"])\n",
    "        \n",
    "        clipwise_ousput_np = to_np(output)\n",
    "        targets_np = to_np(b_data[\"targets\"])\n",
    "        f1_score_3 = metrics.f1_score(targets_np > 0.5, clipwise_ousput_np > 0.3, average=\"samples\")\n",
    "        f1_score_5 = metrics.f1_score(targets_np > 0.5, clipwise_ousput_np > 0.5, average=\"samples\")\n",
    "\n",
    "        self.log(f'Loss/val', loss, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log(f'F1_03/val', f1_score_3, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log(f'F1_05/val', f1_score_5, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x for x in outputs]).mean()\n",
    "        print(f'epoch = {self.current_epoch}, loss = {avg_loss}')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = get_optimizer(self.model)\n",
    "        scheduler = get_scheduler(optimizer)\n",
    "#         optimizer = AdamW(self.model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "#         scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
    "#                                                     num_warmup_steps=self.num_warmup_steps, \n",
    "#                                                     num_training_steps=self.num_train_steps)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"Loss/val\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b79ebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np(input):\n",
    "    return input.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de142b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------AST Model Summary---------------\n",
      "ImageNet pretraining: True, AudioSet pretraining: False\n",
      "frequncey stride=10, time stride=10\n",
      "number of patches=744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.8/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msqrt4kaido\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/sqrt4kaido/BirdCLEF-Experiment/runs/1ud7oys7\" target=\"_blank\">major-shape-66</a></strong> to <a href=\"https://wandb.ai/sqrt4kaido/BirdCLEF-Experiment\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type              | Params\n",
      "------------------------------------------------\n",
      "0 | model     | ASTModel          | 6.0 M \n",
      "1 | criterion | BCEWithLogitsLoss | 0     \n",
      "------------------------------------------------\n",
      "6.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.0 M     Total params\n",
      "23.979    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, loss = 0.7295738458633423\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980e2565c363417c8320f08d724fa2fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, loss = 0.016856366768479347\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1, loss = 0.012423884123563766\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 2, loss = 0.008736757561564445\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 3, loss = 0.007092364132404327\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 4, loss = 0.006062367931008339\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 5, loss = 0.0055192275904119015\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 6, loss = 0.0050773355178534985\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 7, loss = 0.004884339403361082\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 8, loss = 0.004725791048258543\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 9, loss = 0.004674761090427637\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 10, loss = 0.004674761090427637\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 11, loss = 0.004658815450966358\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 12, loss = 0.004660205915570259\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 13, loss = 0.004681472200900316\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 14, loss = 0.004701925441622734\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 15, loss = 0.004796568304300308\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 16, loss = 0.004885914269834757\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 17, loss = 0.00495030777528882\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 18, loss = 0.00498152244836092\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 19, loss = 0.0050038122572004795\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 20, loss = 0.005063019227236509\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 21, loss = 0.0048802439123392105\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 22, loss = 0.004769057966768742\n"
     ]
    }
   ],
   "source": [
    "for i, (trn_idx, val_idx) in enumerate(splitter.split(train, y=train[\"primary_label\"])):\n",
    "    if i not in CFG.folds:\n",
    "        continue\n",
    "\n",
    "    trn_df = train.loc[trn_idx, :].reset_index(drop=True)\n",
    "    val_df = train.loc[val_idx, :].reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    loaders = {\n",
    "        phase: torchdata.DataLoader(\n",
    "            WaveformDataset(\n",
    "                df_,\n",
    "                train_datadir,\n",
    "                img_size=CFG.img_size,\n",
    "                period=CFG.period,\n",
    "                validation=(phase == \"valid\")\n",
    "            ),\n",
    "            **CFG.loader_params[phase])  # type: ignore\n",
    "        for phase, df_ in zip([\"train\", \"valid\"], [trn_df, val_df])\n",
    "    }\n",
    "    \n",
    "    num_train_steps = int(len(loaders['train']) * CFG.epochs)\n",
    "    num_warmup_steps = int(num_train_steps / 10)    \n",
    "    model = ASTModel(label_dim=CFG.num_classes, \n",
    "         input_fdim=CFG.n_mels, \n",
    "         input_tdim=626, \n",
    "         audioset_pretrain=False, \n",
    "         model_size='tiny224', \n",
    "         verbose=True)\n",
    "    model_name = model.__class__.__name__\n",
    "    \n",
    "    learner = Learner(model, num_train_steps, num_warmup_steps)\n",
    "    \n",
    "    # loggers\n",
    "    RUN_NAME = f'exp{str(CFG.exp_num)}'\n",
    "    wandb.init(project='BirdCLEF-Experiment', entity='sqrt4kaido', group=RUN_NAME, job_type=RUN_NAME + f'-fold-{i}')\n",
    "    wandb.run.name = RUN_NAME + f'-fold-{i}'\n",
    "    wandb_config = wandb.config\n",
    "    wandb_config.model_name = model_name\n",
    "    wandb.watch(model)\n",
    "    \n",
    "    # callbacks\n",
    "    callbacks = []\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=f'Loss/val',\n",
    "        mode='min',\n",
    "        dirpath=OUTPUT_DIR,\n",
    "        verbose=False,\n",
    "        filename=f'{model_name}-{learner.current_epoch}-{i}')\n",
    "    callbacks.append(checkpoint_callback)\n",
    "\n",
    "#     early_stop_callback = EarlyStopping(\n",
    "#         monitor='Loss/val',\n",
    "#         min_delta=0.00,\n",
    "#         patience=20,\n",
    "#         verbose=True,\n",
    "#         mode='min')\n",
    "#     callbacks.append(early_stop_callback)\n",
    "    \n",
    "    loggers = []\n",
    "    loggers.append(WandbLogger())\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        logger=loggers,\n",
    "        callbacks=callbacks,\n",
    "        max_epochs=CFG.epochs,\n",
    "        default_root_dir=OUTPUT_DIR,\n",
    "        gpus=1,\n",
    "#         fast_dev_run=DEBUG,\n",
    "        deterministic=True,\n",
    "        benchmark=False,\n",
    "        )\n",
    "    \n",
    "    trainer.fit(learner, train_dataloader=loaders['train'], val_dataloaders=loaders['valid'])\n",
    "    \n",
    "    trainer.save_checkpoint(OUTPUT_DIR / \"last.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf16fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project='BirdCLEF-2021', entity='sqrt4kaido', group=RUN_NAME, job_type='summary')\n",
    "wandb.run.name = 'summary'\n",
    "# wandb.log({'CV_score': oofs_score})\n",
    "wandb.save('./config_.py')\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67545a96-3bd5-4646-ab72-cd64eef6cf6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
